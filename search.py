

# search the best threshold


import torch
from torchvision import transforms
import numpy as np
#from PIL import Image
#from ResNet import resnet18, resnet50, resnet101
import collections
from collections import OrderedDict
import torch.nn.functional as F
import cv2
import os, sys, shutil
import random as rd
from model_irse_224 import IR_50, IR_101, IR_152, IR_SE_50, IR_SE_101, IR_SE_152
from PIL import Image
import numpy as np
import pdb
import torch.nn.functional as F
import torch.utils.data as data
from torch.autograd import Variable
from torch.nn.modules.loss import _WeightedLoss
import time

from sklearn.metrics import precision_recall_curve
from sklearn.utils.extmath import stable_cumsum
from sklearn.metrics import f1_score
import pickle



#from ..utils.extmath import 

os.environ['CUDA_VISIBLE_DEVICES'] = '3'

def load_imgs(image_list_file):
    #import pdb; pdb.set_trace()
    imgs = list()
    with open(image_list_file, 'r') as imf:
        for line in imf:
            # pdb.set_trace()
            line = line.strip()
            line = line.split()
            img_name = line[0]
            label_arr = line[1:]
            img_path = '/home/jisijie/Code/img_dataset_align/' + img_name
            # path, label, dentity_level = line.split(' ',2)
            # label = int(label)
            imgs.append((img_path,label_arr))
            #print(imgs)  #imgs here actually is label of imgs
    return imgs



def load_224_models():
    model_224 = IR_50([224,224]).cuda()
    pretrained_net_dict = torch.load('align_224_model_best.pth.tar')
    new_state_dict = OrderedDict()
    
    for k, v in pretrained_net_dict['state_dict'].items():
        # import pdb; pdb.set_trace()
        name = k[7:] # remove `module.`
        new_state_dict[name] = v
    #import pdb; pdb.set_trace()
    model_224.load_state_dict(new_state_dict, strict = True)
    model_224.eval()
    return model_224


def load_affect_models():
    model_affect_all = IR_50([224,224]).cuda()
    pretrained_net_dict = torch.load('affectnet_model_best.pth.tar')
    new_state_dict = OrderedDict()
    
    for k, v in pretrained_net_dict['state_dict'].items():
        # import pdb; pdb.set_trace()
        name = k[7:] # remove `module.`
        new_state_dict[name] = v
    #import pdb; pdb.set_trace()
    model_affect_all.load_state_dict(new_state_dict, strict = True)
    model_affect_all.eval()
    return model_affect_all

def load_imbalance_models():
    model_imbalance_all = IR_50([224,224]).cuda()
    pretrained_net_dict = torch.load('imbalanced_sample_model_best.pth.tar')
    new_state_dict = OrderedDict()
    
    for k, v in pretrained_net_dict['state_dict'].items():
        # import pdb; pdb.set_trace()
        name = k[7:] # remove `module.`
        new_state_dict[name] = v
    #import pdb; pdb.set_trace()
    model_imbalance_all.load_state_dict(new_state_dict, strict = True)
    model_imbalance_all.eval()
    return model_imbalance_all


def load_152weighted_models():
    model_224_all = IR_152([224,224]).cuda()
    pretrained_net_dict = torch.load('ir152_weighted_loss_64_model_best.pth.tar')
    new_state_dict = OrderedDict()
    
    for k, v in pretrained_net_dict['state_dict'].items():
        # import pdb; pdb.set_trace()
        name = k[7:] # remove `module.`
        new_state_dict[name] = v
    #import pdb; pdb.set_trace()
    model_224_all.load_state_dict(new_state_dict, strict = True)
    model_224_all.eval()
    return model_224_all

def load_152imbalanced_models():
    model_224_all = IR_152([224,224]).cuda()
    pretrained_net_dict = torch.load('ir152_imbalanced_sample_model_best.pth.tar')
    new_state_dict = OrderedDict()
    
    for k, v in pretrained_net_dict['state_dict'].items():
        # import pdb; pdb.set_trace()
        name = k[7:] # remove `module.`
        new_state_dict[name] = v
    #import pdb; pdb.set_trace()
    model_224_all.load_state_dict(new_state_dict, strict = True)
    model_224_all.eval()
    return model_224_all




RGB_MEAN = (0.5, 0.5, 0.5) # for normalize inputs to [-1, 1]
RGB_STD = (0.5, 0.5, 0.5)

class CaffeCrop(object):
    """
    This class take the same behavior as sensenet
    """
    def __init__(self, phase):
        assert(phase=='train' or phase=='test')
        self.phase = phase

    def __call__(self, img):
        # pre determined parameters
        final_size = 112
        final_width = final_height = final_size
        crop_size = 110
        crop_height = crop_width = crop_size
        crop_center_y_offset = 15
        crop_center_x_offset = 0
        if self.phase == 'train':
            scale_aug = 0.02
            trans_aug = 0.01
        else:
            scale_aug = 0.0
            trans_aug = 0.0
        
        # computed parameters
        randint = rd.randint
        scale_height_diff = (randint(0,1000)/500-1)*scale_aug
        crop_height_aug = crop_height*(1+scale_height_diff)
        scale_width_diff = (randint(0,1000)/500-1)*scale_aug
        crop_width_aug = crop_width*(1+scale_width_diff)


        trans_diff_x = (randint(0,1000)/500-1)*trans_aug
        trans_diff_y = (randint(0,1000)/500-1)*trans_aug


        center = ((img.width/2 + crop_center_x_offset)*(1+trans_diff_x),
                 (img.height/2 + crop_center_y_offset)*(1+trans_diff_y))

        
        if center[0] < crop_width_aug/2:
            crop_width_aug = center[0]*2-0.5
        if center[1] < crop_height_aug/2:
            crop_height_aug = center[1]*2-0.5
        if (center[0]+crop_width_aug/2) >= img.width:
            crop_width_aug = (img.width-center[0])*2-0.5
        if (center[1]+crop_height_aug/2) >= img.height:
            crop_height_aug = (img.height-center[1])*2-0.5

        crop_box = (center[0]-crop_width_aug/2, center[1]-crop_height_aug/2,
                    center[0]+crop_width_aug/2, center[1]+crop_width_aug/2)

        mid_img = img.crop(crop_box)
        res_img = img.resize( (final_width, final_height) )
        #import pdb; pdb.set_trace()
        return res_img


def sigmoid(x):
    return 1/(1+np.exp(-x))

def main():
    imgs= load_imgs('label_test_align.txt')
    #imgs_train = load_imgs('label.txt')
    Whole_pre = []
    Whole_gt  = []
    #Train_gt = []
    #import pdb; pdb.set_trace()
    total_cnt = len(imgs)
    #total_cnt_train = len(imgs_train)
 
    # model = load_models()
    # model.eval()
    model_224 = load_224_models()
    model_224.eval()
    model_affect = load_affect_models()
    model_affect.eval()
    model_imbalance = load_imbalance_models()
    model_imbalance.eval()
    model_152imbalanced = load_152imbalanced_models()
    model_152imbalanced.eval()
    model_152_weighted = load_152weighted_models()
    model_152_weighted.eval()
    
    #model_affect = load_affect_models()
    
    #model_affect.eval()

    #print(model.eval())
    for i in range(total_cnt):

        
        #path = imgs[i][0]
        img  = imgs[i][0]
        #import pdb; pdb.set_trace()
        img = Image.open(img).convert("RGB")
             
        # img = cv2.imread(img)
        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        # img = Image.fromarray(img)
    
        #img = cv2.resize(img,(112,112),interpolation=cv2.INTER_CUBIC)
        transform_kw = transforms.Compose([ # refer to https://pytorch.org/docs/stable/torchvision/transforms.html for more build-in online data augmentation
        transforms.Resize([224, 224]), # smaller side resized
    
        transforms.ToTensor(),
        transforms.Normalize(mean = RGB_MEAN,
                              std = RGB_STD),
        ])
        #import pdb; pdb.set_trace()
        img_tensor = transform_kw(img)
        img_tensor = torch.unsqueeze(img_tensor, 0)
        # tic = time.time()
        
        # toc = time.time()
        # print(toc - tic)
        #import pdb; pdb.set_trace()
        
        #preds = model(img_tensor.cuda())    # for single model evaluate
        prediction1 = model_224(img_tensor.cuda())
        prediction2 = model_affect(img_tensor.cuda())
        prediction3 = model_imbalance(img_tensor.cuda())
        prediction4 = model_152imbalanced(img_tensor.cuda())
        prediction5 = model_152_weighted(img_tensor.cuda())
    #prediction = prediction1 
        prediction = (prediction1 + prediction2 + prediction3 + prediction4 + prediction5) / 5
        preds = torch.sigmoid(prediction)
    
        #prediction1 = model_224(img_tensor.cuda())
        #prediction2 = model_affect(img_tensor.cuda())
        #preds = prediction1 
        #preds = (prediction1 + prediction2) / 2

        preds = preds.cpu()
        preds = preds.detach().numpy().flatten()
        #import pdb; pdb.set_trace()
        
        #preds = sigmoid(preds)
       
        gt = imgs[i][1]
        
        
        Whole_pre.append(preds)
        Whole_gt.append(gt)
        print(i)
    
        # #[  34,   96,   99,  173,  177,  181,  190,  227,  283,  319,  328,
    #     331,  378,  442,  494,  503,  552,  576,  582,  601,  632,  717,
    #     813,  835,  840,  848,  864,  905,  943,  965,  985, 1065, 1097,
    #    1182, 1194, 1202, 1223, 1232, 1244, 1267, 1274, 1283, 1286, 1329,
    #    1330, 1334, 1337, 1338, 1345, 1351, 1362, 1383, 1390, 1422, 1425,
    #    1453, 1457, 1471, 1589, 1666, 1669, 1672, 1720, 1785, 1817, 1876,
    #    1910, 1926, 1931, 1932, 1937, 1940, 1942, 1944, 1963, 1996])
    
    
    #import pdb; pdb.set_trace()
    merged_preds = np.array(Whole_pre) # the prediction value
    merged_preds = sigmoid(merged_preds) 

    labels = np.array(Whole_gt) # the grountruth value
    labels = labels.astype(np.int)
    labels = labels.tolist()
    #import pdb; pdb.set_trace()
    f1_optimal_thresholds = []    
    for q in range(23):


        col = [x[q] for x in labels] # extract col 
        col = np.array(col).astype(np.int)
        idx = np.argwhere(col == 999).flatten()
        col = [i for i in col if i != 999]
   

        col_pre = np.array(merged_preds[:,q]).flatten()
        col_pre = col_pre.tolist()
        
        cnt = 0
        for i in range(len(idx)):
            col_pre.pop(idx[i]-cnt)
        #import pdb; pdb.set_trace()
            cnt = cnt + 1

        #import pdb; pdb.set_trace()
        precision, recall, thresholds = precision_recall_curve(col,col_pre)
        idx = np.argwhere(thresholds>0.49)
        index_of_first = idx.min()
        F1_thres = 2 * (precision[index_of_first] * recall[index_of_first]) / (precision[index_of_first] + recall[index_of_first])
        
        bst_index = np.abs(precision-recall).argmin(0)
        F1_best = 2 * (precision[bst_index] * recall[bst_index]) / (precision[bst_index] + recall[bst_index])
        # col_pre_tra = np.array(col_pre)
        # col_pre_tra[col_pre_tra<0.5] =0
        # col_pre_tra[col_pre_tra>=0.5] =1
        # F1 = f1_score(col,col_pre_tra)
        #import pdb; pdb.set_trace()
        # print('F1',F1)
        # print('F1_thres',F1_thres)
        # print('*F1_best',F1_best)

        #np.abs(precision-recall).argmin(0)

        # 
        # col_pre[col_pre<0.5] =0
        # col_pre[col_pre>=0.5] =1
       # F1 = 

        #accuracy_score(target, pred_score)
        #import pdb; pdb.set_trace()
        f1_optimal_thresholds.append(thresholds[np.abs(precision-recall).argmin(0)])
    
    print(f1_optimal_thresholds)
        


    #merged_preds = sigmoid(preds)
    # best_thresholds_over_models = []

    # for i in range(len(merged_preds)): # 2100
    #     f1_optimal_thresholds = []
    #     merged_preds_per_model = merged_preds[i] 
    #     for j in range(merged_preds_per_model.shape[1]): # 23 AU
    #         import pdb; pdb.set_trace()
    #         precision, recall, thresholds = precision_recall_curve(labels[i][:,j].astype(np.int),merged_preds[i][:,j])
    #         f1_optimal_thresholds.append(thresholds[np.abs(precision-recall).argmin(0)])
    #     f1_optimal_thresholds = np.array(f1_optimal_thresholds)
    # best_thresholds_over_models.append(f1_optimal_thresholds)
    # best_thresholds_over_models = np.array(best_thresholds_over_models).mean(0)
    # merged_preds = np.mean(merged_preds, axis=0) 
    # merged_preds = merged_preds > (np.ones_like(merged_preds)*best_thresholds_over_models)
    # merged_preds = merged_preds.astype(np.int64)
    # print("The best AU thresholds over models: {}".format(best_thresholds_over_models))
    
    # for j in range(total_cnt_train):
              
    #     gt_train = imgs[j][1]
    #     import pdb; pdb.set_trace()
    #     Train_gt.append(gt_train)

        # Fx = prediction
        # Fy = gt
        # #compute Avg F score
        # TP = ((Fx == 1) & (Fy == 1)).sum()
        #     # TN    predict 和 label 同时为
        # TN = ((Fx == 0) & (Fy == 0)).sum()
        # # FN    predict 0 label 1
        # FN = ((Fx == 0) & (Fy == 1)).sum()
        # # FP    predict 1 label 0
        # FP = ((Fx == 1) & (Fy == 0)).sum()
        # Fpre  = TP.float() / (TP.float() + FP.float())
        # Frec = TP.float() / (TP.float() + FN.float())
        # F05 = (1+0.25) * Fpre * Frec / (0.25*Fpre + Frec)
        # F1 = (1+1) * Fpre * Frec / (1*Fpre + Frec)
        # F2 = (1+4) * Fpre * Frec / (4*Fpre + Frec)
        # AvgF = ( F05 + F1 + F2 ) / 3
        # #prediction[prediction<=0.5] =0
        # #prediction[prediction>0.5] =1


    # for k in range(len(prediction)):
    #     f1_optimal_thresholds = []
    #     merged_preds_per_model = merged_preds[i]
    #     for j in range(merged_preds_per_model.shape[1]):
    #         precision, recall, thresholds = precision_recall_curve(labels[i][:, j].astype(np.int),merged_preds[i][:, j])
    #         f1_optimal_thresholds.append(thresholds[np.abs(precision-recall).argmin(0)])
    #         f1_optimal_thresholds = np.array(f1_optimal_thresholds)


    #print(Whole_gt.shape())



def main_test():

    col = [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
    col_sl = [0,0,0,1,0,1,1]
    
    col_pre_sl = [0.07712152600288391, 0.03873132914304733, 0.14778800308704376,0.2063405066728592, 0.026862360537052155, 0.021272724494338036, 0.21241217851638794]
    precision, recall, thresholds = precision_recall_curve(col_sl,col_pre_sl)
    col_pre = [0.03912587836384773, 0.05996948108077049, 0.12268661707639694, 0.03158717602491379, 0.278794527053833, 0.2498922199010849, 0.022304782643914223, 0.1023893877863884, 0.0034103940706700087, 0.040482763200998306, 0.08007215708494186, 0.05510608106851578, 0.005261678248643875, 0.06460914015769958, 0.07476034760475159, 0.0492817759513855, 0.025171903893351555, 0.004810563288629055, 0.009280907921493053, 0.009389122016727924, 0.1564408838748932, 0.25711801648139954, 0.03389521315693855, 0.16607031226158142, 0.09437519311904907, 0.016746368259191513, 0.13170146942138672, 0.008878682740032673, 0.008148174732923508, 0.005802543368190527, 0.22117123007774353, 0.07848493754863739, 0.004660809878259897, 0.13969522714614868, 0.07006506621837616, 0.09129485487937927, 0.00333058531396091, 0.00452800840139389, 0.01091985497623682, 0.04921363294124603, 0.032963018864393234, 0.004990431945770979, 0.06674736738204956, 0.051186129450798035, 0.07664290070533752, 0.14811542630195618, 0.20567496120929718, 0.13914749026298523, 0.018050897866487503, 0.010969852097332478, 0.18446741998195648, 0.0932086706161499, 0.05597221478819847, 0.03495296835899353, 0.009422474540770054, 0.33073025941848755, 0.6562837362289429, 0.050749197602272034, 0.007700400426983833, 0.03802453726530075, 0.21898892521858215, 
0.07712152600288391, 0.03873132914304733, 0.14778800308704376, 0.025555642321705818, 0.00748985493555665, 0.01962856948375702, 0.005002645310014486, 0.0008339737541973591, 0.08770047873258591, 0.01327485591173172, 0.08286965638399124, 0.06609933078289032, 0.022149302065372467, 0.05321333929896355, 0.008840357884764671, 0.033290985971689224, 0.0012220056960359216, 0.0631617084145546, 0.01783551089465618, 0.02829349972307682, 0.024815764278173447, 0.05525297299027443, 0.01911727525293827, 0.03359206020832062, 0.0405852235853672, 0.031082363799214363, 0.05257563292980194, 0.0907011479139328, 0.04594511538743973, 0.09505701810121536, 0.07568664103746414, 0.4243605434894562, 0.22843025624752045, 0.1181354969739914, 0.138787642121315, 0.1396804004907608, 0.010154478251934052, 0.06548053026199341, 0.04319780692458153, 0.09357069432735443, 0.010692881420254707, 0.01038922555744648, 0.009054891765117645, 0.11251389235258102, 0.02002071961760521, 0.028684377670288086, 0.11616441607475281, 0.0017856438644230366, 0.0034942610654979944, 0.015095156617462635, 0.0057625980116426945, 0.23419728875160217, 0.0164013858884573, 0.12893711030483246, 0.010594899766147137, 0.029073238372802734, 0.008246484212577343, 0.002552816877141595, 0.013076795265078545, 0.10665370523929596, 0.06359964609146118, 0.028375890105962753, 0.03018052875995636, 0.08152519166469574, 0.014610650949180126, 0.009842121042311192, 0.011215424165129662, 0.053719937801361084, 0.02962837740778923, 0.005296521354466677, 0.03201545029878616, 0.04525156691670418, 0.1610674411058426, 0.18654923141002655, 0.005454238969832659, 0.2420487254858017, 0.08813725411891937, 0.025773750618100166, 0.006469994317740202, 0.013423889875411987, 0.11254742741584778, 0.27524250745773315, 0.01485705841332674, 0.07227467745542526, 0.0025264518335461617, 0.009715094231069088, 0.015233593061566353, 0.0411628820002079, 0.017139429226517677, 0.029164357110857964, 0.041128985583782196, 0.03246263787150383, 0.04608577489852905, 0.08635172247886658, 0.02847357653081417, 0.002514046151190996, 0.060381099581718445, 0.0017511826008558273, 0.018888898193836212, 0.039910029619932175, 0.08494924753904343, 0.07463708519935608, 0.0036060453858226538, 0.003729014191776514, 0.09268689155578613, 0.004559030290693045, 
0.00805629976093769, 0.05633978918194771, 0.007524876389652491, 0.016058437526226044, 0.014296402223408222, 0.011930233798921108, 0.07367247343063354, 0.22568398714065552, 0.007334611378610134, 0.429909884929657, 0.0013965759426355362, 0.0174991674721241, 0.18370814621448517, 0.16346780955791473, 0.025123318657279015, 0.03516378626227379, 0.010163841769099236, 0.01938379742205143, 0.002076877048239112, 0.09336065500974655, 0.11078713834285736, 0.013052184134721756, 0.1498260498046875, 0.10698357969522476, 0.12715807557106018, 0.2550298869609833, 0.016566859558224678, 0.044375523924827576, 0.025284988805651665, 0.004186029080301523, 0.04399806633591652, 0.028560511767864227, 0.016196079552173615, 0.021509071812033653, 0.05066236853599548, 0.08961708098649979, 0.072598397731781, 0.1375867873430252, 0.18581722676753998, 0.1050049290060997, 0.001355712185613811, 0.252959281206131, 0.06152581050992012, 0.04768954962491989, 0.0921454131603241, 0.0025985813699662685, 0.01943262666463852, 0.09696108102798462, 0.10584362596273422, 0.06621890515089035, 0.017359968274831772, 0.2056209146976471, 0.00713658332824707, 0.05177760496735573, 0.14350776374340057, 0.03795265778899193, 0.023811519145965576, 0.009129769168794155, 0.25467702746391296, 0.04234125465154648, 0.02144283428788185, 0.037058599293231964, 0.11093942075967789, 0.02666548639535904, 0.037136275321245193, 0.5036994814872742, 0.03662105277180672, 0.012865102849900723, 0.003937069326639175, 0.04131351411342621, 0.025389201939105988, 0.060332559049129486, 0.05746006220579147, 0.04688804969191551, 0.2771143913269043, 0.17879168689250946, 0.029926370829343796, 0.0024996353313326836, 0.023626942187547684, 0.008214342407882214, 0.07308898121118546, 0.004962819628417492, 0.03913075476884842, 0.021282553672790527, 0.06371069699525833, 0.015876153483986855, 0.18266460299491882, 0.07553227245807648, 0.07285729795694351, 0.018905339762568474, 0.05966408550739288, 0.00899294763803482, 0.004885517060756683, 0.12032260745763779, 0.009405078366398811, 0.015408838167786598, 0.03937607258558273, 0.030542505905032158, 0.03542878106236458, 0.08274926245212555, 0.0633140355348587, 0.04525502771139145, 0.3190288245677948, 0.04870053380727768, 0.0062891095876693726, 0.06511281430721283, 0.010497086681425571, 0.07233551889657974, 0.01456239353865385, 0.03463512659072876, 0.016696123406291008, 0.13289302587509155, 0.01637857034802437, 0.012502185069024563, 0.0793297067284584, 0.006815989501774311, 0.03746179863810539, 0.20577917993068695, 0.09494739025831223, 0.06541261821985245, 0.001562863471917808, 0.008387514390051365, 0.035188447684049606, 0.09678459912538528, 0.027706053107976913, 0.03759220987558365, 0.008672709576785564, 0.01559591107070446, 0.014594649896025658, 0.04708048328757286, 0.08451005816459656, 0.05731986463069916, 0.3794373869895935, 0.026578029617667198, 0.04394005984067917, 0.11869296431541443, 0.008668807335197926, 0.04528557136654854, 0.014323263429105282, 0.009684034623205662, 0.1361541450023651, 0.04142940789461136, 0.01400412805378437, 0.01404201053082943, 0.005573473405092955, 0.06986384093761444, 0.16885606944561005, 0.024355489760637283, 0.009032255969941616, 0.03642433136701584, 0.16637708246707916, 0.39818015694618225, 0.06386641412973404, 0.12359374761581421, 0.029454577714204788, 0.06950602680444717, 0.04991171881556511, 0.024643544107675552, 0.06386641412973404, 0.00875095371156931, 0.03917698189616203, 0.002658906625583768, 0.024205729365348816, 0.06146916002035141, 0.014484814368188381, 0.16502313315868378, 0.10466179996728897, 0.18135619163513184, 0.004067710600793362, 0.01791270449757576, 0.021333329379558563, 0.01299849059432745, 0.016338832676410675, 0.029176980257034302, 0.01531938835978508, 
0.12547513842582703, 0.2063405066728592, 0.026862360537052155, 0.021272724494338036, 0.21241217851638794, 0.02536146529018879, 0.1334463655948639, 0.004489927552640438, 0.3251519501209259, 0.030212407931685448, 0.0009853992378339171, 0.03522865101695061, 0.3058024048805237, 0.06340359896421432, 0.03585955128073692, 0.06314229220151901, 0.03947700560092926, 0.004738995339721441, 0.12449654191732407, 0.2521013617515564, 0.022475704550743103, 0.14936751127243042, 0.10136105865240097, 
0.07016295194625854, 0.14583982527256012, 0.09821835160255432, 0.012203223071992397, 0.05326598137617111, 0.003911745734512806, 0.004151913803070784, 0.03889269009232521, 0.02049708552658558, 0.05041034519672394, 0.07296211272478104, 0.2315940260887146, 0.08971240371465683, 0.01342595461755991, 0.056469328701496124, 0.0018101707100868225, 0.04185328260064125, 0.0076155466958880424, 0.1346328854560852, 
0.048565447330474854, 0.01159964594990015, 0.08180414885282516, 0.033517494797706604, 0.028603460639715195, 0.2574375867843628, 0.03252439945936203, 0.13756884634494781, 0.02440059185028076, 0.05395031347870827, 0.008074642159044743, 0.021836956962943077, 0.028326472267508507, 0.004831702448427677, 0.010809063911437988, 0.04470216482877731, 0.014798715710639954, 0.04319780692458153, 0.011384280398488045, 0.01003737561404705, 0.002326690824702382, 0.02783745527267456, 0.0034449428785592318, 0.08070812374353409, 0.016417371109128, 0.03679328411817551, 0.06737148016691208, 0.007269760128110647, 0.0011881443206220865, 0.002952347509562969, 0.004398930352181196, 0.022242486476898193, 0.03569924086332321, 0.04394005984067917, 0.0041511389426887035, 0.04355759173631668, 0.033187128603458405, 0.008028496988117695, 0.025372738018631935, 0.21289724111557007, 0.014193780720233917, 0.019635755568742752, 0.002003204543143511, 0.085047647356987, 0.1470402330160141, 0.21553078293800354, 0.007710051257163286, 0.04684525728225708, 0.07401455193758011, 0.09021643549203873, 0.1865595132112503, 0.05780438706278801, 0.004719249438494444, 0.09491453319787979, 0.05678758770227432, 0.004101681523025036, 0.024756092578172684, 0.010891181416809559, 0.015408838167786598, 0.05187174305319786, 0.08459866046905518, 0.03473243489861488, 0.0179291944950819, 0.13474953174591064, 0.06278296560049057, 0.0048760888166725636, 0.24420247972011566, 0.0047177537344396114, 0.05228063836693764, 0.0019948207773268223, 0.13232555985450745, 0.010592739097774029, 0.039711952209472656, 0.03415512666106224, 0.021898427978157997, 0.04622514918446541, 0.03673165291547775, 0.0867273136973381, 0.05875922366976738, 0.04344840347766876, 0.09933875501155853, 0.09556936472654343, 0.03361362963914871, 0.058014146983623505, 0.023321764543652534, 0.17400303483009338, 0.17371998727321625, 0.09449348598718643, 0.07938273251056671, 0.22054137289524078, 0.06638629734516144, 0.014146760106086731, 0.006423581391572952, 0.053713954985141754, 0.013486914336681366, 0.016430264338850975, 0.04847963526844978, 0.021591516211628914, 0.03406516835093498, 0.1589900553226471, 0.16862043738365173, 0.031189603731036186, 0.032514479011297226, 0.034654855728149414, 0.07876387983560562, 0.0442654974758625, 
0.030212407931685448, 0.0017461085226386786, 0.10458995401859283, 0.08006298542022705, 0.0077570965513587, 0.07044526189565659, 0.016034118831157684, 0.013452591374516487, 0.03125307708978653, 0.05559685453772545, 0.03642433136701584, 0.030811740085482597, 0.0021279894281178713, 0.04494607076048851, 0.03440532460808754, 0.060390084981918335, 0.028309980407357216, 0.09475640952587128, 0.04567253589630127, 0.04448142275214195, 0.054424434900283813, 0.022799916565418243, 0.04689691215753555, 0.0071145701222121716, 0.006137460935860872, 0.025200234726071358, 0.014279383234679699, 0.020937569439411163, 0.017202746123075485, 0.02713119611144066, 
0.019358143210411072, 0.029454577714204788, 0.1387249082326889, 0.056863438338041306, 0.026350947096943855, 0.02503792569041252, 0.06445580720901489, 0.012435580603778362, 0.08182628452777863, 0.050529610365629196, 0.005401148926466703, 0.0006708282744511962, 0.5220526456832886, 0.06830844283103943, 0.06291503459215164, 0.013298280537128448, 0.0037555741146206856, 0.21511587500572205, 0.006328043062239885, 0.0924547091126442, 0.07316768914461136, 0.059315118938684464, 0.06629627197980881, 0.000590556999668479, 0.03658989444375038, 0.005844120401889086, 0.04528557136654854, 0.36428317427635193, 0.009675104171037674, 0.02000236324965954, 0.12348893284797668, 0.07636060565710068, 0.000777804700192064, 0.011306812055408955, 0.010240324772894382, 0.03382378816604614, 0.08709660172462463, 0.03170907869935036, 0.18463249504566193, 0.034693606197834015, 0.0357375405728817, 0.023070426657795906, 0.07382579892873764, 0.19075776636600494, 0.020612193271517754, 0.13953238725662231, 0.0073831393383443356, 0.04405130818486214, 0.09106933325529099, 0.06497252732515335, 0.06545571237802505, 0.03341782093048096, 0.013674219138920307, 0.012206488288939, 0.06336373090744019, 0.03686762973666191, 0.010138601064682007, 0.0021984735503792763, 0.00737400958314538, 0.043365877121686935, 0.018647177144885063, 0.0017167903715744615, 0.03430456295609474, 0.03260074928402901, 0.22886906564235687, 0.10481705516576767, 0.04378667101264, 0.12209958583116531, 0.16700860857963562, 0.006118407007306814, 0.05405547469854355, 0.007578148040920496, 0.07525383681058884, 0.10848802328109741, 0.015501085668802261, 0.03744424507021904, 0.007672000210732222, 0.05096198618412018, 0.00935827661305666, 0.0016360272420570254, 0.10450341552495956, 0.025057129561901093, 0.15671859681606293, 0.041501596570014954, 0.11402210593223572, 0.2217024266719818, 0.02039787918329239, 0.016656775027513504, 0.035483330488204956, 0.12216663360595703, 0.0015449889469891787, 0.007180932909250259, 0.028163179755210876, 0.10075616091489792, 0.09365436434745789, 0.017830191180109978, 0.05338694155216217, 0.10037524253129959, 0.013364525511860847, 0.025471597909927368, 0.03509994223713875, 0.04015958681702614, 0.029064197093248367, 0.040845006704330444, 0.13578295707702637, 0.40158459544181824, 
0.00957708153873682, 0.0019949281122535467, 0.008573898114264011, 0.040763769298791885, 0.06032363697886467, 0.020850975066423416, 0.009505711495876312, 0.06522665172815323, 0.02211390808224678, 0.08672238141298294, 0.00877355132251978, 0.0234215147793293, 0.023043446242809296, 0.008282056078314781, 0.011597301810979843, 0.08111679553985596, 0.003781681414693594, 0.02914142981171608, 0.5526331663131714, 0.02141333930194378, 0.014357981272041798, 0.001836207928135991, 0.010749103501439095, 0.00378437340259552, 0.08482732623815536, 0.024437978863716125, 0.1677890121936798, 0.015668539330363274, 0.008308211341500282, 0.002233310369774699, 0.016027389094233513, 0.028653929010033607, 0.0033204166684299707, 0.08208054304122925, 0.0198807455599308, 0.15209059417247772, 0.04387900233268738, 0.014612299390137196, 0.03954765200614929, 0.16673550009727478, 0.0269154142588377, 0.008949932642281055, 0.027275579050183296, 0.14281713962554932, 0.03741765022277832, 0.03028714284300804, 0.04006157070398331, 0.05666781961917877, 0.2317395955324173, 0.02995827980339527, 0.038339532911777496, 0.011037738062441349, 0.2942129671573639, 0.030406875535845757, 0.03270832821726799, 0.10631850361824036, 0.019193794578313828, 0.027570806443691254, 0.006416333373636007, 0.008183999918401241, 0.014197484590113163, 0.09377220273017883, 0.09694882482290268, 0.13344785571098328, 0.1572522222995758, 0.13294287025928497, 0.04023206979036331, 0.24378745257854462, 0.15217293798923492, 0.0069872294552624226, 0.12269105017185211, 0.022347088903188705, 
0.023120561614632607, 0.011164167895913124, 0.0038780593313276768, 0.04454539343714714, 0.034914273768663406, 0.0013748601777479053, 0.08765067160129547, 0.1030130609869957, 0.06142335385084152, 0.16186489164829254, 0.09597086906433105, 0.02960830181837082, 0.07898455858230591, 0.006292728241533041, 0.019182417541742325, 0.005950065329670906, 0.06294045597314835, 0.04060635343194008, 0.08823046833276749, 0.27090325951576233, 0.0182329174131155, 0.04631422087550163, 0.00729351956397295, 0.01178867556154728, 0.013839242979884148, 0.11825700849294662, 0.26012110710144043, 0.07058239728212357, 0.0286299716681242, 0.15604795515537262, 0.05297291651368141, 0.190529927611351, 0.010256891138851643, 0.17024366557598114, 0.04763892665505409, 0.021443964913487434, 0.02934497408568859, 0.05698508769273758, 0.0036849072203040123, 0.004908178932964802, 0.023500947281718254, 0.0409088134765625, 0.005387308541685343, 0.10958567261695862, 0.1413559764623642, 0.14583982527256012, 0.05391043797135353, 0.334827721118927, 0.1161058247089386, 0.06122234836220741, 0.09114664793014526, 0.08369407802820206, 0.2375653088092804, 0.05916309356689453, 0.07117845118045807, 0.024237295612692833, 0.10713373124599457, 0.06734011322259903, 0.04544830694794655, 0.04119094833731651, 0.024332940578460693, 0.040976349264383316, 0.05971517413854599, 0.1229233667254448, 0.16170020401477814, 0.08318915963172913, 0.09422659128904343, 0.08885122835636139, 0.06704788655042648, 
0.030977193266153336, 0.03894900530576706, 0.11816953867673874, 0.1537538468837738, 0.03708585724234581, 0.005589084234088659, 0.019616624340415, 0.041195567697286606, 0.021954262629151344, 0.05531222000718117, 0.002489253878593445, 0.0883280336856842, 0.003745191264897585, 0.16947278380393982, 0.05135749280452728, 0.09474409371614456, 0.043829359114170074, 0.04230642691254616, 0.004160044714808464, 0.3004627823829651, 0.012100089341402054, 0.05805040895938873, 0.02403849922120571, 0.5224171876907349, 0.1646164208650589, 0.16523553431034088, 0.11640623956918716, 0.07299394905567169, 0.0009868681663647294, 0.016192324459552765, 0.025582537055015564, 0.014222506433725357, 0.04855544492602348, 0.034498102962970734, 0.04965902864933014, 0.02886108309030533, 0.02962120994925499, 0.013767056167125702, 0.003389804158359766, 0.020408030599355698, 0.04296500235795975, 0.12618552148342133, 0.01347306091338396, 0.038388632237911224, 0.026784580200910568, 0.09064406901597977, 0.0035155368968844414, 0.008146715350449085, 0.31120702624320984, 0.02956383489072323, 0.03609165549278259, 0.0038137079682201147, 0.024612780660390854, 0.016237521544098854, 0.005538199562579393, 0.0023753864225000143, 0.04275922477245331, 0.02420744113624096, 0.0032619235571473837, 0.004673739429563284, 0.26934629678726196, 0.5816431641578674, 0.00200216774828732, 0.0892505869269371, 0.0885443240404129, 0.25640568137168884, 0.07227779924869537, 0.008833538740873337, 0.11004290729761124, 0.04313487932085991, 0.0019337503472343087, 0.004556881729513407, 
0.01903240941464901, 0.1002274602651596, 0.04686923697590828, 0.14832794666290283, 0.04165056720376015, 0.007169091608375311, 0.17924988269805908, 0.024825839325785637, 0.05440628156065941, 0.01230914331972599, 0.09098836779594421, 0.07519537210464478, 0.16573220491409302, 0.0419807992875576, 0.002719427924603224, 0.012149550952017307, 0.09391472488641739, 0.01867835409939289, 0.018361246213316917, 0.03607194125652313, 0.035436492413282394, 0.003822173224762082, 0.01858513429760933, 0.047131530940532684, 0.007322466466575861, 0.06816967576742172, 0.04570910334587097, 0.02705451287329197, 0.05442691594362259, 0.1226780042052269, 0.021048318594694138, 0.0016451786505058408, 0.002823226386681199, 0.01161154918372631, 0.020570486783981323, 0.023529110476374626, 0.05633311718702316, 0.03391120582818985, 
0.04221733659505844, 0.0006420044228434563, 0.0005141247529536486, 0.1493951976299286, 0.11478639394044876, 0.16033370792865753, 0.00452902028337121, 0.08155443519353867, 0.019955076277256012, 0.13044603168964386, 0.023261841386556625, 0.06209281459450722, 0.11411487311124802, 0.23026971518993378, 0.02487826719880104, 0.023997846990823746, 0.01122624333947897, 0.0007585844141431153, 0.02532181143760681, 0.05798514559864998, 0.01964576356112957, 0.018183033913373947, 0.08044935762882233, 0.013823620043694973, 0.09893608838319778, 0.4534956216812134, 0.09424958378076553, 0.05602721497416496, 0.025991715490818024, 0.01466849260032177, 0.08006952702999115, 0.00577523373067379, 0.10272126644849777, 0.15136639773845673, 0.02980807237327099, 0.0693020224571228, 0.0046258061192929745, 0.06871217489242554, 0.03172427788376808, 0.0060684154741466045, 0.003454019082710147, 0.08768541365861893, 0.006200733594596386, 0.09863242506980896, 0.15396079421043396, 0.017642108723521233, 0.11681651324033737, 0.051908835768699646, 0.08165424317121506, 0.0014994043158367276, 0.11314860731363297, 0.057497281581163406, 0.0025301664136350155, 0.022453295066952705, 0.011475501582026482, 0.023284539580345154, 0.3590852916240692, 0.027926504611968994, 0.03910220414400101, 0.0067179217003285885, 0.13148662447929382, 0.040574584156274796, 0.07026303559541702, 0.003109642304480076, 0.1671513468027115, 0.010867367498576641, 0.020924564450979233, 0.0347219817340374, 0.011871464550495148, 0.03161241486668587, 0.04777468740940094, 0.07504671812057495, 0.009583931416273117, 0.025285271927714348, 0.07861924171447754, 0.06904604285955429, 0.015665045008063316, 0.13683496415615082, 0.09400216490030289, 0.015092453919351101, 0.036573778837919235, 0.41876649856567383, 0.022214731201529503, 0.11281821131706238, 0.08673467487096786, 0.2773677408695221, 0.04402575269341469, 0.21530017256736755, 0.00926752295345068, 0.03479054570198059, 0.055564139038324356, 0.001467309077270329, 0.013028157874941826, 0.08161956071853638, 0.2918996214866638, 0.24508216977119446, 0.08592421561479568, 0.14602771401405334, 0.1835918426513672, 0.04067298397421837, 0.07736796140670776, 0.03724629804491997, 0.28909188508987427, 0.13901257514953613, 0.06598521769046783, 0.046669404953718185, 0.617039680480957, 0.08228068798780441, 0.0027649584226310253, 0.002767062745988369, 0.017780739814043045, 0.012728453613817692, 0.021938838064670563, 0.025881236419081688, 0.02634638547897339, 0.09249687194824219, 0.202764630317688, 0.04326719418168068, 0.11212646961212158, 0.05626047030091286, 0.00403931038454175, 0.009701171889901161, 0.011763869784772396, 0.19337423145771027, 0.17105728387832642, 0.05073137208819389, 0.011766081675887108, 0.014965568669140339, 0.0814925879240036, 0.03842053934931755, 0.06756436079740524, 0.0027566547505557537, 0.0024590527173131704, 0.07135487347841263, 0.019937073811888695, 0.05189935863018036, 0.008454403840005398, 0.16056552529335022, 0.009900770150125027, 0.07249698042869568, 0.007868795655667782, 0.0077047995291650295, 0.004692059475928545, 0.009039172902703285, 0.013221913017332554, 0.012244182638823986, 0.010751946829259396, 0.0035711077507585287, 0.007838050834834576, 0.012384320609271526, 0.01663328893482685, 0.00992863904684782, 0.12401673197746277, 0.02070298045873642, 0.19530372321605682, 0.007591278292238712, 0.11626965552568436, 0.11626965552568436, 0.0036432889755815268, 0.1281006783246994, 0.002495214343070984, 0.16707943379878998, 0.034810006618499756, 0.09466813504695892, 0.01049971766769886, 0.11685629934072495, 0.20671910047531128, 0.07633012533187866, 0.07634872198104858, 0.045304544270038605, 0.04077394679188728, 0.01423370186239481, 0.3787110149860382, 0.03769320994615555, 0.02051546424627304, 0.05642838776111603, 0.12167053669691086, 0.05363278463482857, 0.06040077283978462, 0.007729826960712671, 0.06755727529525757, 0.1637490689754486, 
0.031875111162662506, 0.12251540273427963, 0.12070928514003754, 0.006460987962782383, 0.03332431986927986, 0.10811200737953186, 0.012186512351036072, 0.01091868244111538, 0.06836249679327011, 0.003822629339993, 0.0028679505921900272, 0.1309426724910736, 0.17854547500610352, 0.37355324625968933, 0.13040392100811005, 0.06995169073343277, 0.0009227633127011359, 0.0026381737552583218, 0.0031504747457802296, 0.02896914631128311, 0.022123116999864578, 0.0113726407289505, 0.0077959392219781876, 0.013271552510559559, 0.058537859469652176, 0.03275420516729355, 0.11485031247138977, 0.02544882334768772, 0.01139436848461628, 0.015224882401525974, 0.05558178201317787, 0.05000047758221626, 0.022867070510983467, 0.004096651449799538, 
0.013098573312163353, 0.004151415079832077, 0.0024204992223531008, 0.0008671794785186648, 0.00900085549801588, 0.022246021777391434, 0.012618226930499077, 0.056193772703409195, 0.0034816039260476828, 0.3168731927871704, 0.013418851420283318, 0.004273959435522556, 0.010674046352505684, 0.0394657626748085, 0.008093723095953465, 0.27118387818336487, 0.005092105828225613, 0.004977503791451454, 0.09787437319755554, 0.012619737535715103, 0.019117973744869232, 0.050678789615631104, 0.015381264500319958, 0.09378210455179214, 0.015694476664066315, 0.0447034053504467, 0.020466402173042297, 0.015890074893832207, 0.017197029665112495, 0.036431849002838135, 0.015891216695308685, 0.07808853685855865, 0.004929821938276291, 0.05973761901259422, 0.08047676831483841, 0.03065132349729538, 0.004344947636127472, 0.049815911799669266, 0.015567607246339321, 0.02017776481807232, 0.013520956039428711, 0.4918058216571808, 0.0027291972655802965, 0.010021676309406757, 0.0018240120261907578, 0.061102453619241714, 0.0165247805416584, 0.14616790413856506, 0.1399402916431427, 0.020728053525090218, 0.018428370356559753, 0.042690128087997437, 0.016482478007674217, 0.01509575080126524, 0.0022143186070024967, 0.008145127445459366, 
0.4101727306842804, 0.003047717735171318, 0.032451581209897995, 0.019777264446020126, 0.005479397252202034, 0.009995175525546074, 0.19888557493686676, 0.010923770256340504, 0.043903712183237076, 0.023179244250059128, 0.16490472853183746, 0.0346783772110939, 0.06556489318609238, 0.03097563423216343, 0.0016146416310220957, 0.0317825973033905, 0.02061973325908184, 0.10405091196298599, 0.0763426348567009, 
0.0038726567290723324, 0.020437993109226227, 0.15585248172283173, 0.011381409130990505, 0.06489326804876328, 0.002089157933369279, 0.001904901466332376, 0.09850452095270157, 0.0068380883894860744, 0.021471578627824783, 0.09069901704788208, 0.01666361093521118, 0.061438556760549545, 0.005935315974056721, 0.019909320399165154, 0.018371520563960075, 0.017223620787262917, 0.030576083809137344, 0.016938067972660065, 0.003598499344661832, 0.11983796954154968, 0.04673192277550697, 0.05290311202406883, 0.028334926813840866, 0.0236586332321167, 0.03626009449362755, 0.008640914224088192, 0.00471889553591609, 0.010231968015432358, 0.05463719740509987, 0.23823922872543335, 0.23686583340168, 0.018596207723021507, 0.020836932584643364, 0.06722448766231537, 0.2711001932621002, 0.1775582879781723, 0.018010195344686508, 0.08279687166213989, 0.015333501622080803, 0.09310607612133026, 0.03602231293916702, 0.014147883281111717, 0.005089372396469116, 0.0020585269667208195, 0.025665555149316788, 0.007613684516400099, 0.00255810865201056, 0.07754262536764145, 
0.006979366298764944, 0.0025429839733988047, 0.18707026541233063, 0.11412513256072998, 0.06408078223466873, 0.01687738671898842, 0.023146843537688255, 0.002283235313370824, 0.07489874958992004, 0.08152267336845398, 0.2413649559020996, 0.11288724839687347, 0.05289993807673454, 0.06143118441104889, 0.08672147244215012, 0.003146414877846837, 0.043798353523015976, 0.004356705117970705, 0.14144498109817505, 0.002557821571826935, 0.027867628261446953, 0.02088220976293087, 0.0022853906266391277, 0.01911882311105728, 0.06204284727573395, 0.07379890233278275, 0.07250836491584778, 0.1934310495853424, 0.10837160050868988, 0.05479516088962555, 0.001102958805859089, 0.0015793730271980166, 0.007349217310547829, 0.0031985128298401833, 0.0662662535905838, 0.15947234630584717, 0.015054537914693356, 0.009114614687860012, 0.022233538329601288, 0.01520479191094637, 0.09725161641836166, 0.010640150867402554, 0.0005009820451959968, 0.05839917063713074, 0.12476079165935516, 0.17250052094459534, 0.1293957531452179, 0.0778847336769104, 0.0158741045743227, 0.012005268596112728, 0.0036779954098165035, 0.012850839644670486, 0.004747425206005573, 0.002352138515561819, 0.08362540602684021, 0.12242016941308975, 0.11345177888870239, 0.01409877184778452, 0.002678768476471305, 0.016688348725438118, 0.0036534240934997797, 0.08543707430362701, 0.2403586208820343, 0.028447719290852547, 0.055783193558454514, 0.020997678861021996, 0.008941520936787128, 0.027961399406194687, 0.014605632983148098, 0.07695013284683228, 0.0371888242661953, 0.09827109426259995, 0.06585926562547684, 0.062224239110946655, 0.03415149077773094, 0.05221327766776085, 0.07360309362411499, 0.009414858184754848, 0.41281625628471375, 0.09167328476905823, 0.0017068447778001428, 0.1001623272895813, 0.032900620251894, 0.07522135972976685, 0.007360378280282021, 0.062219180166721344, 0.11541088670492172, 0.05581899359822273, 0.009695768356323242, 0.03164983540773392, 0.049506254494190216, 0.02961253561079502, 0.01777753420174122, 0.2998083233833313, 0.0025834613479673862, 0.016282493248581886, 0.02708953060209751, 0.0637182667851448, 0.17310994863510132, 0.004150691907852888, 0.029548611491918564, 0.07031042873859406, 0.005311381537467241, 0.03892447054386139, 0.014485979452729225, 0.009385361336171627, 0.036377567797899246, 0.04550826922059059, 0.006396227981895208, 0.03823041543364525, 0.13543154299259186, 0.03244486078619957, 0.0033332679886370897, 0.055411197245121, 0.004083008039742708, 0.08068875223398209, 0.023252978920936584, 0.00538532342761755, 0.022293949499726295, 0.08078991621732712, 0.03049635700881481, 0.09805215895175934, 0.019687602296471596, 0.022650523111224174, 0.02240423858165741, 0.08273931592702866, 0.0027276300825178623, 0.0019546099938452244, 0.008424636907875538, 0.0009458771673962474, 0.0039903693832457066, 0.00787543598562479, 0.006168740801513195, 0.0034303604625165462, 0.020072033628821373, 0.0016455152072012424, 0.07519173622131348, 0.0069642141461372375, 0.10014411062002182, 0.004755323752760887, 0.004207203164696693, 0.32508671283721924, 0.11514871567487717, 0.014815508387982845, 0.01506076194345951, 0.09253698587417603, 0.009783515706658363, 0.005299974232912064, 0.0031507278326898813, 0.013284508138895035, 0.1366119235754013, 0.01922558806836605, 0.0069521451368927956, 0.016810771077871323, 0.002828358905389905, 0.026532864198088646, 0.0070001776330173016, 0.12692692875862122, 0.06011969596147537, 0.014788421802222729, 0.095485158264637, 0.026974383741617203, 
0.04347698763012886, 0.08866751194000244, 0.046277400106191635, 0.01862018182873726, 0.00920832995325327, 0.07420957833528519, 0.03554559126496315, 0.06975466012954712, 0.3633454144001007, 0.33549124002456665, 0.26335957646369934, 0.03695229813456535, 0.05210688337683678, 0.006905560381710529, 0.021230701357126236, 0.06915709376335144, 0.005891909822821617, 0.27355828881263733, 0.01652168110013008, 0.08811867982149124, 0.017016014084219933, 0.026830516755580902, 0.017565729096531868, 0.047737106680870056, 0.017199013382196426, 0.05363775044679642, 0.10661008954048157, 0.011590847745537758, 0.041284918785095215, 0.01213426236063242, 0.0582839660346508, 0.041895024478435516, 0.035199280828237534, 0.027122149243950844, 0.00045105148456059396, 0.017315348610281944, 0.0020920655224472284, 0.0077422792091965675, 0.31396549940109253, 0.005944215692579746, 0.0018790934700518847, 0.07863583415746689, 0.01857520267367363, 0.01217701192945242, 0.025604158639907837, 0.021369965746998787, 0.041220344603061676, 0.027448279783129692, 0.011204209178686142, 0.014415765181183815, 0.04574911668896675, 0.0066884970292449, 0.03772755712270737, 0.4167118966579437, 0.018289834260940552, 0.038029108196496964, 0.044976212084293365, 0.024793259799480438, 0.019431710243225098, 0.039512474089860916, 0.10861527919769287, 0.010463940910995007, 0.1304527223110199, 0.004484701901674271, 0.07300432026386261, 0.006632696837186813, 0.04625570774078369, 0.11400003731250763, 0.027225311845541, 0.025887439027428627, 0.0036029438488185406, 0.1071566790342331, 0.16310720145702362, 0.11023809760808945, 0.01388875674456358, 0.014701317995786667, 0.013446720317006111, 0.037584565579891205, 0.0044211652129888535, 0.006541678216308355, 0.01464114896953106, 0.012996521778404713, 0.01331702433526516, 0.003011391032487154, 0.012818703427910805, 0.001977609470486641, 0.061425354331731796, 0.05182531103491783, 0.10374502092599869, 0.07856085151433945, 0.13238753378391266, 0.000536946696229279, 0.0034905073698610067, 0.03893619030714035, 0.045033104717731476, 0.12730300426483154, 0.0014457004144787788, 0.017562150955200195, 0.018556322902441025, 0.002475683344528079, 0.03121032752096653, 0.021956391632556915, 0.019580159336328506, 0.0166329313069582, 0.05334832891821861, 0.016854826360940933, 0.0029294518753886223, 0.04059101641178131, 0.032216425985097885, 
0.011907462030649185, 0.0485001839697361, 0.01419839821755886, 0.013313529081642628, 0.0037004677578806877, 0.006195331923663616, 0.1001262366771698, 0.009408517740666866, 0.03433883935213089, 0.027403520420193672, 0.012614449486136436, 0.0999452993273735, 0.013384572230279446, 0.16281913220882416, 0.008234886452555656, 0.015642480924725533, 0.04784303903579712, 0.012410569936037064, 0.04315531253814697, 0.051595818251371384, 0.14225584268569946, 0.0700937956571579, 0.01532942708581686, 0.0838029533624649, 0.17178364098072052, 0.30676373839378357, 0.2969740629196167, 0.09541743993759155, 0.23690512776374817, 0.006063838489353657, 0.008684664033353329, 0.05192894861102104, 0.13539259135723114, 0.01659638248383999, 0.09616541862487793, 0.031641554087400436, 0.012206912972033024, 0.03742338716983795, 0.10166484862565994, 0.008778097108006477, 0.027328167110681534, 0.04187918081879616, 0.0999738872051239, 0.08000414818525314, 0.1839873045682907, 0.015792852267622948, 0.007620716001838446, 0.005751847289502621, 0.046017635613679886, 0.15578272938728333, 0.01578262634575367, 0.01905292272567749, 0.023279178887605667, 0.016094649210572243, 0.29404202103614807, 0.11001016944646835, 0.05315842479467392, 0.00547063909471035, 0.011788547970354557, 0.03489488735795021, 0.038861121982336044, 0.00410544453188777, 0.01443017553538084, 0.08421299606561661, 0.0036748687271028757, 0.04886837303638458, 0.010246643796563148, 0.009891435503959656, 0.15578913688659668, 0.10554282367229462, 0.0033658016473054886, 0.139822319149971, 0.022168809548020363, 0.012612652033567429, 0.044709231704473495, 0.1764974445104599, 0.050394028425216675, 0.05728864297270775, 0.0068541900254786015, 0.019187234342098236, 0.02352755330502987, 0.024405214935541153, 0.08476191014051437, 0.011603041552007198, 0.02430291473865509, 0.03304928168654442, 0.024975668638944626, 0.038511667400598526, 0.012120609171688557, 0.07327616959810257, 0.011313684284687042, 0.021621040999889374, 0.02338576689362526, 0.007268149871379137, 0.04790768027305603, 0.026866652071475983, 0.04315759986639023, 0.03641442954540253, 0.003783807158470154, 0.027278929948806763, 0.04434288293123245, 0.04740649089217186, 0.009596799500286579, 0.015142064541578293, 0.039555735886096954, 0.029373647645115852, 0.028257599100470543, 0.01295197568833828, 0.016145313158631325, 0.1558917611837387, 0.023946573957800865, 0.011602429673075676, 0.5738670825958252, 0.015598636120557785, 0.0035090248566120863, 0.0037827466148883104, 0.024686574935913086, 0.0008323018555529416, 0.020167648792266846, 0.005988669581711292, 0.023910164833068848, 0.04500169679522514, 0.026635268703103065, 0.00929715670645237, 0.05554857477545738, 0.2342543751001358, 0.02036064863204956, 0.017858028411865234, 0.09376832097768784, 0.09746528416872025, 0.014219525270164013, 0.020481331273913383, 0.06758887320756912, 0.06739843636751175, 0.10991248488426208, 0.011124787852168083, 0.0007317024283111095, 0.005137741565704346, 0.03845878690481186, 0.06949647516012192, 0.04540733993053436, 0.06638772040605545, 0.07472868263721466, 0.008358157239854336, 0.22305764257907867, 0.36810994148254395, 0.1433677226305008, 0.02496345341205597, 0.06507755815982819, 0.017408495768904686, 0.15735957026481628, 0.007632986642420292, 0.007047612220048904, 0.03265491873025894, 0.02245676890015602, 0.03584834933280945, 0.003508947789669037, 0.006737049203366041, 0.04525962471961975, 0.08139728754758835, 0.06695778667926788, 0.05322366952896118, 0.14531299471855164, 0.04046574607491493, 0.031700145453214645, 0.011791053228080273, 0.06377453356981277, 0.04569878801703453, 0.1456388533115387, 0.028832627460360527, 0.11822011321783066, 0.03457130119204521, 0.024864470586180687, 0.02413971722126007, 
0.005013556219637394, 0.134283646941185, 0.008405235596001148, 0.07430779188871384, 0.1912682205438614, 0.009482300840318203, 0.04092314839363098, 0.014888322912156582, 0.05660150945186615, 0.005093942396342754, 0.06313670426607132, 0.1421283483505249, 0.005954432301223278, 0.03245727717876434, 0.006273647304624319, 0.013961256481707096, 0.02026015892624855, 0.06128082796931267, 0.1735713630914688, 0.11234620213508606, 0.38235342502593994, 0.004491508938372135, 0.01677839271724224, 0.007459552027285099, 0.011463635601103306, 0.009425287134945393, 0.04084555432200432, 0.08802662789821625, 0.08745471388101578, 0.01753544807434082, 0.0444314107298851, 0.005109715741127729, 0.02792760170996189, 0.20119115710258484, 0.030718348920345306, 0.103180430829525, 0.02626752108335495, 0.23699235916137695, 0.046461071819067, 0.06279306858778, 0.09578195959329605, 0.006752360612154007, 0.07807862013578415, 0.0006044938927516341, 0.06302724033594131, 0.024546515196561813, 0.0715644508600235, 0.050719089806079865, 0.1246255412697792, 0.05387807637453079, 0.10455415397882462, 0.38107389211654663, 0.06511605530977249, 0.07360511273145676, 0.0019049812108278275, 0.0007193383062258363, 0.006101675797253847, 0.15263967216014862, 0.01416060607880354, 0.06344985961914062, 0.15318839251995087, 0.19086633622646332, 0.029531842097640038, 0.19196535646915436, 0.007850420661270618, 0.005176851060241461, 0.010550410486757755, 0.07330843061208725, 0.03592533990740776, 0.031430065631866455, 0.05949766933917999, 0.004169674124568701, 0.006717809941619635, 0.03665895760059357, 0.014984460547566414, 0.10972225666046143, 0.017199190333485603, 0.019451655447483063, 0.017565729096531868, 0.021682696416974068, 
0.20661313831806183, 0.05724906921386719, 0.16489000618457794, 0.011352472938597202, 0.15725882351398468, 0.07169989496469498, 0.050361569970846176, 0.04528604820370674, 0.16434678435325623, 0.07832410931587219, 0.12325248122215271, 0.04463190957903862, 0.09154292196035385, 0.021184829995036125, 0.015445573255419731, 0.003927316516637802, 0.0030186509247869253, 0.05416681244969368, 0.13536784052848816, 0.03033370152115822, 0.007427142467349768, 0.46142110228538513, 0.008252976462244987, 0.007559973746538162, 0.007450435776263475, 0.00036461924901232123, 0.03600526228547096, 0.011574744246900082, 0.051653891801834106, 0.07250846177339554, 0.019267885014414787, 0.0023777377791702747, 0.26666417717933655, 0.21879905462265015, 0.17039524018764496, 0.026899583637714386, 0.2936651110649109, 0.04276049882173538, 0.015144340693950653, 0.014202008955180645, 0.2591923475265503, 0.001620167982764542, 0.0912129357457161, 0.028193708509206772, 0.03244565799832344, 0.11413708329200745, 0.10522754490375519, 0.00560792488977313, 0.05617361515760422, 0.020175527781248093, 0.05784837156534195, 0.03593776375055313, 0.036783572286367416, 0.0028966953977942467, 0.026576055213809013, 0.07599859684705734, 0.12143603712320328, 0.018313592299818993, 0.13424751162528992, 0.013634635135531425, 0.003830681787803769, 0.02735850214958191, 0.10889320075511932, 0.017263539135456085, 0.05475188419222832, 0.029174886643886566, 0.021058734506368637, 0.07690645754337311, 0.06484508514404297, 0.08045780658721924, 0.004177343100309372, 0.09454458206892014, 0.15329180657863617, 0.08031994849443436, 0.015336603857576847, 0.0051330882124602795, 0.09220170974731445, 0.003481814172118902, 0.04454037919640541, 0.022677680477499962, 0.0059310379438102245, 0.014436585828661919, 0.028477743268013, 
0.01035500317811966, 0.01750803180038929, 0.014726067893207073, 0.005737918429076672, 0.006460302975028753, 0.1660429686307907, 0.011727876029908657, 0.0200672410428524, 0.005911906249821186, 0.04480741545557976, 0.00583154521882534, 0.021351894363760948, 0.0014578867703676224, 0.01967760920524597, 0.04034281522035599, 0.01715487614274025, 0.009648408740758896, 0.03740772604942322, 0.058917898684740067, 0.05603571981191635, 0.0002808306599035859, 0.02665018104016781, 0.016626646742224693, 0.008498657494783401, 0.1567351520061493, 0.056522827595472336, 0.0013531162403523922, 0.050871219485998154, 0.013443329371511936, 0.022239312529563904, 0.04552855342626572, 0.012917786836624146, 0.026891980320215225, 0.01292125042527914, 0.017755582928657532, 0.058612532913684845, 0.06232593208551407, 0.02886374481022358, 0.07437001913785934, 0.12352583557367325, 0.1461440771818161, 0.0021739015355706215, 0.018576107919216156, 0.12287745624780655, 0.022002283483743668, 0.009709952399134636, 0.0927547737956047, 0.02639731764793396, 0.2054324895143509, 0.012636706233024597, 0.05359086021780968, 0.02088475227355957, 0.01152653619647026, 0.26168292760849, 0.0008850733283907175, 0.11087039858102798, 0.10190173983573914, 0.01051480881869793, 0.060955554246902466, 0.00393583532422781, 0.020708782598376274, 0.027528829872608185, 0.04667821153998375, 0.06128031387925148, 0.08450987190008163, 0.21240970492362976, 0.023657916113734245, 0.0023147298488765955, 0.41581201553344727, 0.06772245466709137, 0.09720000624656677, 0.02716233767569065, 0.01729731634259224, 0.024188930168747902, 0.14010372757911682, 0.11247436702251434, 0.05215690657496452, 0.0036308339331299067, 0.00987951084971428, 0.22778618335723877, 0.04400398209691048, 0.1297140270471573, 0.051920853555202484, 0.052694857120513916, 0.05507507920265198, 0.01727251335978508, 0.01791643723845482, 0.09402549266815186, 0.043686218559741974, 0.19153337180614471, 0.0021350872702896595, 0.040820784866809845, 0.013456477783620358, 0.04843544214963913, 0.005282689351588488, 0.0014618962304666638, 0.018651679158210754, 0.24533270299434662, 0.7985780239105225, 0.07132125645875931]

    #fps, tps, thresholds = _binary_clf_curve(y_true, y_score, pos_label=None)    
    #tps, thresholds = _binary_clf_curve_cst(col_sl, col_pre_sl, pos_label=None)
    
    #import pdb; pdb.set_trace()



def accuracy_score(y_true, y_pred, normalize=True, sample_weight=None):
    """Accuracy classification score.
    In multilabel classification, this function computes subset accuracy:
    the set of labels predicted for a sample must *exactly* match the
    corresponding set of labels in y_true.
    Read more in the :ref:`User Guide <accuracy_score>`.
    Parameters
    ----------
    y_true : 1d array-like, or label indicator array / sparse matrix
        Ground truth (correct) labels.
    y_pred : 1d array-like, or label indicator array / sparse matrix
        Predicted labels, as returned by a classifier.
    normalize : bool, optional (default=True)
        If ``False``, return the number of correctly classified samples.
        Otherwise, return the fraction of correctly classified samples.
    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.
    Returns
    -------
    score : float
        If ``normalize == True``, return the fraction of correctly
        classified samples (float), else returns the number of correctly
        classified samples (int).
        The best performance is 1 with ``normalize == True`` and the number
        of samples with ``normalize == False``.
    See also
    --------
    jaccard_score, hamming_loss, zero_one_loss
    Notes
    -----
    In binary and multiclass classification, this function is equal
    to the ``jaccard_score`` function.
    Examples
    --------
    >>> from sklearn.metrics import accuracy_score
    >>> y_pred = [0, 2, 1, 3]
    >>> y_true = [0, 1, 2, 3]
    >>> accuracy_score(y_true, y_pred)
    0.5
    >>> accuracy_score(y_true, y_pred, normalize=False)
    2
    In the multilabel case with binary label indicators:
    >>> import numpy as np
    >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
    0.5
    """

    # Compute accuracy for each possible representation
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    check_consistent_length(y_true, y_pred, sample_weight)
    if y_type.startswith('multilabel'):
        differing_labels = count_nonzero(y_true - y_pred, axis=1)
        score = differing_labels == 0
    else:
        score = y_true == y_pred

    return _weighted_sum(score, sample_weight, normalize)


# def main():
    
#     f=open('/home/jisijie/Code/align_gt.txt','rb')
#     Whole_gt=pickle.load(f)
#     print(Whole_gt)
#     f.close()

#     f=open('/home/jisijie/Code/align_preds.txt','rb')
#     Whole_pre=pickle.load(f)
#     print(Whole_pre)
#     f.close()
    
#     for i in range(23):
#         import pdb; pdb.set_trace()
#         threshold= np.arange(0.0,1.0,0.1)


        


if __name__ == '__main__':
    main()

 #raise ValueError("y_true takes value in {{{classes_repr}}} and " "pos_label is not specified: either make y_true ""take value in {{0, 1}} or {{-1, 1}} or ""pass pos_label explicitly.".format(classes_repr=classes_repr))